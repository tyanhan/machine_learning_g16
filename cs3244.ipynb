{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab4acb7a-64dd-4107-9ee4-d0ee582724f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6dff234-7a99-4b49-88af-0a40d98ef085",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import glob\n",
    "\n",
    "file = glob.glob('./clothing-dataset-small-master.zip')\n",
    "\n",
    "with zipfile.ZipFile(file[0], 'r') as zip_ref:\n",
    "    zip_ref.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc0f5491-21de-454f-ab7e-d7dcc901a7df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for data_type in [\"train\", \"validation\", \"test\"]:\n",
    "    folder_path = \"./data/clothing-dataset-small-master/\" + data_type\n",
    "    \n",
    "    # combine validation and train\n",
    "    new_folder_path = \"./data/clothing-dataset/\" + (\"test\" if data_type == \"test\" else \"train\")\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        new_path = os.path.join(new_folder_path, folder)\n",
    "        if not os.path.exists(new_path):\n",
    "            os.makedirs(new_path)\n",
    "\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                new_image_path = os.path.join(new_folder_path, folder, filename)\n",
    "                img.save(new_image_path)\n",
    "                                              \n",
    "                # flip all images besides t-shirts to balance the data\n",
    "                if folder != \"t-shirt\":\n",
    "                    img = img.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "                    new_image_path = os.path.join(new_folder_path, folder, \"flipped_\" + filename)\n",
    "                    img.save(new_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98320d99-d645-4720-97f3-7980eac9159f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature: Grayscale\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for data_type in [\"train\", \"test\"]:\n",
    "    folder_path = \"./data/clothing-dataset/\" + data_type\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            # Open the image\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                img = Image.open(image_path)\n",
    "                # resize the image\n",
    "                img = img.resize((64, 64))\n",
    "                # convert to grayscale\n",
    "                img = img.convert('L')\n",
    "                # flatten to 1D array\n",
    "                array = np.array(img).ravel()\n",
    "                \n",
    "                if data_type == \"test\":\n",
    "                    X_test.append(array)\n",
    "                    y_test.append(folder)\n",
    "                else:\n",
    "                    X_train.append(array)\n",
    "                    y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7dd1fb93-9d11-40a8-823f-cc4556c427c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from skimage.io import imread, imshow\n",
    "from skimage import transform\n",
    "from skimage.filters import prewitt\n",
    "\n",
    "def preprocess_image_edge_only(image):\n",
    "    resized_image = transform.resize(image, (64, 64), anti_aliasing=True)\n",
    "    edges_prewitt = prewitt(resized_image)\n",
    "    edges_prewitt_array = edges_prewitt.reshape(1, 64 * 64)\n",
    "    return edges_prewitt_array[0]\n",
    "\n",
    "def preprocess_image(image):\n",
    "    resized_image = transform.resize(image, (64, 64), anti_aliasing=True)\n",
    "    edges_prewitt = prewitt(resized_image)\n",
    "    edges_prewitt_array = edges_prewitt.reshape(1, 64 * 64)\n",
    "    image_array = resized_image.reshape(1, 64 * 64)\n",
    "    return np.concatenate((edges_prewitt_array[0], image_array[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a22c5e0-d7bd-44b6-8275-fbc68bafb929",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature: Edges\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "image = None\n",
    "for data_type in data_types:\n",
    "    folder_path = \"./data/clothing-dataset/\" + data_type\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            # Open the image\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                image = imread(image_path,as_gray=True)\n",
    "                result = preprocess_image_edge_only(image)\n",
    "                if data_type == \"test\":\n",
    "                    X_test.append(result)\n",
    "                    y_test.append(folder)\n",
    "                else:\n",
    "                    X_train.append(result)\n",
    "                    y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df749708-8851-430d-8acc-94382341a1d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature: Grayscale + Edges\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "data_types = [\"train\", \"test\"]\n",
    "\n",
    "image = None\n",
    "count = 0\n",
    "for data_type in data_types:\n",
    "    folder_path = \"./data/clothing-dataset/\" + data_type\n",
    "\n",
    "    for folder in os.listdir(folder_path):\n",
    "        for filename in os.listdir(os.path.join(folder_path, folder)):\n",
    "            # Open the image\n",
    "            if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "                image_path = os.path.join(folder_path, folder, filename)\n",
    "                image = imread(image_path,as_gray=True)\n",
    "                result = preprocess_image(image)\n",
    "                if data_type == \"test\":\n",
    "                    X_test.append(result)\n",
    "                    y_test.append(folder)\n",
    "                else:\n",
    "                    X_train.append(result)\n",
    "                    y_train.append(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf0446c4-811f-4794-bb3d-4847852e8a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN:\n",
      "Best k:  {'n_neighbors': 2}\n",
      "Best F1 score:  0.532310914018231\n",
      "Test F1 Score:  0.342485549132948\n",
      "DT:\n",
      "Best depth:  {'max_depth': 14}\n",
      "Best F1 score:  0.3749590097781855\n",
      "Test F1 Score:  0.2976878612716763\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, make_scorer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# define the number of folds for cross validation\n",
    "NUM_FOLDS = 5\n",
    "\n",
    "# define param search space for knn and dt\n",
    "knn_param_grid = {'n_neighbors': list(range(2, 100))}\n",
    "dt_param_grid = {'max_depth': list(range(2, 100))}\n",
    "\n",
    "# define models\n",
    "knn = KNeighborsClassifier()\n",
    "dt = DecisionTreeClassifier(max_features=512)\n",
    "\n",
    "# define scoring metric\n",
    "f1_scorer = make_scorer(f1_score, average='micro')\n",
    "\n",
    "# KNN\n",
    "knn_model = RandomizedSearchCV(knn, knn_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, n_jobs=-1)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"KNN:\")\n",
    "print(\"Best k: \", knn_model.best_params_)\n",
    "print(\"Best F1 score: \", knn_model.best_score_)\n",
    "\n",
    "knn_final = KNeighborsClassifier(n_neighbors=int(knn_model.best_params_['n_neighbors']))\n",
    "knn_final.fit(X_train, y_train)\n",
    "y_pred = knn_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "# DT\n",
    "dt_model = RandomizedSearchCV(dt, dt_param_grid, cv=NUM_FOLDS, scoring=f1_scorer, n_jobs=-1)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "print(\"DT:\")\n",
    "print(\"Best depth: \", dt_model.best_params_)\n",
    "print(\"Best F1 score: \", dt_model.best_score_)\n",
    "\n",
    "dt_final = DecisionTreeClassifier(max_depth=int(dt_model.best_params_['max_depth']), max_features=512)\n",
    "dt_final.fit(X_train, y_train)\n",
    "y_pred = dt_final.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e0dc30f-255e-4cc7-b19d-df305fc3f58c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test F1 Score:  0.30346820809248554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tayya\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:702: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# SVM\n",
    "svc = make_pipeline(StandardScaler(), SGDClassifier(max_iter=3000))\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "print(\"Test F1 Score: \", f1_score(y_test, y_pred, average='micro'))\n",
    "\n",
    "# svm_model = RandomizedSearchCV(svc, param_distributions=param_dist, cv=2, scoring=f1_scorer, n_jobs=-1, n_iter=2)\n",
    "# svm_model.fit(X_train, y_train)\n",
    "\n",
    "# print(\"SVM\")\n",
    "# print(\"Best params: \", svm_model.best_params_)\n",
    "# print(\"Best F1 score: \", svm_model.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bf54f51-fcf0-4f0e-a7ec-d2aedee04476",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 41s 413ms/step - loss: 2.1834 - accuracy: 0.2405 - val_loss: 2.1149 - val_accuracy: 0.3109\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 39s 409ms/step - loss: 1.9802 - accuracy: 0.3357 - val_loss: 1.7376 - val_accuracy: 0.3900\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 43s 452ms/step - loss: 1.7275 - accuracy: 0.4280 - val_loss: 1.5541 - val_accuracy: 0.4545\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 40s 414ms/step - loss: 1.5385 - accuracy: 0.4889 - val_loss: 1.4582 - val_accuracy: 0.5484\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 45s 471ms/step - loss: 1.4185 - accuracy: 0.5251 - val_loss: 1.2826 - val_accuracy: 0.5953\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 39s 405ms/step - loss: 1.3279 - accuracy: 0.5515 - val_loss: 1.2631 - val_accuracy: 0.6012\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 40s 414ms/step - loss: 1.2227 - accuracy: 0.5834 - val_loss: 1.2198 - val_accuracy: 0.6041\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 39s 403ms/step - loss: 1.1487 - accuracy: 0.6141 - val_loss: 1.0912 - val_accuracy: 0.6217\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 41s 431ms/step - loss: 1.0999 - accuracy: 0.6274 - val_loss: 1.0607 - val_accuracy: 0.6393\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 43s 453ms/step - loss: 1.0569 - accuracy: 0.6522 - val_loss: 1.0009 - val_accuracy: 0.6540\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 44s 454ms/step - loss: 0.9741 - accuracy: 0.6714 - val_loss: 0.9795 - val_accuracy: 0.6716\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 42s 434ms/step - loss: 0.9379 - accuracy: 0.6890 - val_loss: 0.9414 - val_accuracy: 0.6891\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - 46s 477ms/step - loss: 0.9167 - accuracy: 0.6982 - val_loss: 1.0318 - val_accuracy: 0.6745\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - 41s 428ms/step - loss: 0.9039 - accuracy: 0.7001 - val_loss: 0.9460 - val_accuracy: 0.7038\n",
      "Epoch 15/20\n",
      "96/96 [==============================] - 41s 432ms/step - loss: 0.8588 - accuracy: 0.7089 - val_loss: 0.9116 - val_accuracy: 0.6979\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - 38s 392ms/step - loss: 0.8331 - accuracy: 0.7187 - val_loss: 0.8854 - val_accuracy: 0.7243\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - 37s 386ms/step - loss: 0.7804 - accuracy: 0.7435 - val_loss: 0.8301 - val_accuracy: 0.7302\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - 41s 431ms/step - loss: 0.7427 - accuracy: 0.7529 - val_loss: 0.8942 - val_accuracy: 0.7038\n",
      "Epoch 19/20\n",
      "96/96 [==============================] - 38s 392ms/step - loss: 0.7246 - accuracy: 0.7480 - val_loss: 0.8977 - val_accuracy: 0.6833\n",
      "Epoch 20/20\n",
      "96/96 [==============================] - 37s 381ms/step - loss: 0.7210 - accuracy: 0.7552 - val_loss: 0.8112 - val_accuracy: 0.7478\n",
      "12/12 [==============================] - 2s 142ms/step - loss: 0.8430 - accuracy: 0.7204\n",
      "Test accuracy: 0.7204301357269287\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(150, 150), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(150, 150), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(150, 150), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"f1_micro\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9401736-7552-459b-8d9b-63869e800fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 21s 212ms/step - loss: 1.9610 - accuracy: 0.3400 - val_loss: 1.6978 - val_accuracy: 0.3988\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 1.4673 - accuracy: 0.5042 - val_loss: 1.4080 - val_accuracy: 0.5191\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 1.2247 - accuracy: 0.5818 - val_loss: 1.2586 - val_accuracy: 0.5689\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 1.0491 - accuracy: 0.6460 - val_loss: 1.2827 - val_accuracy: 0.5513\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 18s 190ms/step - loss: 0.9278 - accuracy: 0.6907 - val_loss: 0.9829 - val_accuracy: 0.6364\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 19s 194ms/step - loss: 0.8826 - accuracy: 0.7063 - val_loss: 1.1609 - val_accuracy: 0.6334\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.7642 - accuracy: 0.7383 - val_loss: 1.0878 - val_accuracy: 0.6598\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.7151 - accuracy: 0.7555 - val_loss: 0.9067 - val_accuracy: 0.6716\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.6949 - accuracy: 0.7637 - val_loss: 0.8920 - val_accuracy: 0.6804\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 19s 200ms/step - loss: 0.6545 - accuracy: 0.7764 - val_loss: 0.8118 - val_accuracy: 0.7038\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.5867 - accuracy: 0.8012 - val_loss: 0.7997 - val_accuracy: 0.7507\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.5809 - accuracy: 0.8022 - val_loss: 0.8337 - val_accuracy: 0.7097\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - 18s 192ms/step - loss: 0.5053 - accuracy: 0.8220 - val_loss: 1.0273 - val_accuracy: 0.6891\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.4516 - accuracy: 0.8380 - val_loss: 0.8369 - val_accuracy: 0.7419\n",
      "Epoch 15/20\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.4485 - accuracy: 0.8422 - val_loss: 0.9575 - val_accuracy: 0.7155\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.4106 - accuracy: 0.8559 - val_loss: 0.8646 - val_accuracy: 0.7185\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.4107 - accuracy: 0.8589 - val_loss: 0.8360 - val_accuracy: 0.7478\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 0.3638 - accuracy: 0.8722 - val_loss: 1.1098 - val_accuracy: 0.7185\n",
      "Epoch 19/20\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.3398 - accuracy: 0.8872 - val_loss: 0.9488 - val_accuracy: 0.7331\n",
      "Epoch 20/20\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.2868 - accuracy: 0.8963 - val_loss: 0.9439 - val_accuracy: 0.7595\n",
      "12/12 [==============================] - 2s 144ms/step - loss: 0.9177 - accuracy: 0.7204\n",
      "Test accuracy: 0.7204301357269287\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1578cd6-4639-44a2-a331-56dfa9638354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/20\n",
      "96/96 [==============================] - 22s 210ms/step - loss: 1.9642 - accuracy: 0.3471 - val_loss: 1.6166 - val_accuracy: 0.4956\n",
      "Epoch 2/20\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 1.5112 - accuracy: 0.5075 - val_loss: 1.4422 - val_accuracy: 0.5279\n",
      "Epoch 3/20\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 1.2771 - accuracy: 0.5851 - val_loss: 1.2183 - val_accuracy: 0.5865\n",
      "Epoch 4/20\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 1.1393 - accuracy: 0.6284 - val_loss: 1.1376 - val_accuracy: 0.6246\n",
      "Epoch 5/20\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 1.0352 - accuracy: 0.6568 - val_loss: 1.0991 - val_accuracy: 0.6540\n",
      "Epoch 6/20\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.9748 - accuracy: 0.6799 - val_loss: 1.0666 - val_accuracy: 0.6393\n",
      "Epoch 7/20\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.8952 - accuracy: 0.7057 - val_loss: 1.0929 - val_accuracy: 0.6510\n",
      "Epoch 8/20\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.8290 - accuracy: 0.7177 - val_loss: 0.9263 - val_accuracy: 0.6921\n",
      "Epoch 9/20\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.7529 - accuracy: 0.7458 - val_loss: 1.1464 - val_accuracy: 0.6364\n",
      "Epoch 10/20\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 0.7303 - accuracy: 0.7575 - val_loss: 1.0225 - val_accuracy: 0.6716\n",
      "Epoch 11/20\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.6896 - accuracy: 0.7637 - val_loss: 1.0135 - val_accuracy: 0.6833\n",
      "Epoch 12/20\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.6405 - accuracy: 0.7787 - val_loss: 1.1124 - val_accuracy: 0.6657\n",
      "Epoch 13/20\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.6072 - accuracy: 0.7989 - val_loss: 0.9554 - val_accuracy: 0.6950\n",
      "Epoch 14/20\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 0.5541 - accuracy: 0.8110 - val_loss: 0.9589 - val_accuracy: 0.7009\n",
      "Epoch 15/20\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.5018 - accuracy: 0.8295 - val_loss: 0.8981 - val_accuracy: 0.7331\n",
      "Epoch 16/20\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.5161 - accuracy: 0.8279 - val_loss: 1.0573 - val_accuracy: 0.6950\n",
      "Epoch 17/20\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.4459 - accuracy: 0.8546 - val_loss: 1.0766 - val_accuracy: 0.7038\n",
      "Epoch 18/20\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.4217 - accuracy: 0.8556 - val_loss: 1.0779 - val_accuracy: 0.6979\n",
      "Epoch 19/20\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.4111 - accuracy: 0.8556 - val_loss: 1.0034 - val_accuracy: 0.7361\n",
      "Epoch 20/20\n",
      "96/96 [==============================] - 20s 212ms/step - loss: 0.3527 - accuracy: 0.8843 - val_loss: 1.0208 - val_accuracy: 0.7038\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 1.1983 - accuracy: 0.6747\n",
      "Test accuracy: 0.6747311949729919\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=20, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ebeaeb7-127c-4c0c-a88d-79c1cbcae78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 22s 218ms/step - loss: 2.0659 - accuracy: 0.2885 - val_loss: 1.8212 - val_accuracy: 0.3548\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 21s 213ms/step - loss: 1.5825 - accuracy: 0.4658 - val_loss: 1.5177 - val_accuracy: 0.4545\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 1.3180 - accuracy: 0.5538 - val_loss: 1.3685 - val_accuracy: 0.5572\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 1.0995 - accuracy: 0.6304 - val_loss: 1.1001 - val_accuracy: 0.6246\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 22s 234ms/step - loss: 0.9618 - accuracy: 0.6744 - val_loss: 0.9935 - val_accuracy: 0.6657\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.8988 - accuracy: 0.6910 - val_loss: 1.0189 - val_accuracy: 0.6393\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.8399 - accuracy: 0.7119 - val_loss: 0.8921 - val_accuracy: 0.6774\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.7430 - accuracy: 0.7507 - val_loss: 0.9614 - val_accuracy: 0.6862\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.7133 - accuracy: 0.7510 - val_loss: 0.9105 - val_accuracy: 0.6862\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 23s 240ms/step - loss: 0.6339 - accuracy: 0.7832 - val_loss: 0.8831 - val_accuracy: 0.6891\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.6134 - accuracy: 0.7976 - val_loss: 0.8485 - val_accuracy: 0.7126\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.5662 - accuracy: 0.7995 - val_loss: 1.0407 - val_accuracy: 0.6745\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.5271 - accuracy: 0.8165 - val_loss: 0.8036 - val_accuracy: 0.7331\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.4742 - accuracy: 0.8344 - val_loss: 1.0002 - val_accuracy: 0.7067\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.4685 - accuracy: 0.8367 - val_loss: 0.8976 - val_accuracy: 0.7155\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 23s 236ms/step - loss: 0.4382 - accuracy: 0.8497 - val_loss: 0.9380 - val_accuracy: 0.7126\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.4084 - accuracy: 0.8585 - val_loss: 1.3738 - val_accuracy: 0.6393\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.3960 - accuracy: 0.8540 - val_loss: 0.8311 - val_accuracy: 0.7361\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.3683 - accuracy: 0.8709 - val_loss: 1.0019 - val_accuracy: 0.7067\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.3396 - accuracy: 0.8765 - val_loss: 1.0289 - val_accuracy: 0.7273\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.3263 - accuracy: 0.8872 - val_loss: 1.1999 - val_accuracy: 0.7038\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 22s 230ms/step - loss: 0.2881 - accuracy: 0.8996 - val_loss: 1.1540 - val_accuracy: 0.7009\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 22s 231ms/step - loss: 0.2778 - accuracy: 0.9074 - val_loss: 1.0747 - val_accuracy: 0.7331\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.2954 - accuracy: 0.8928 - val_loss: 0.9908 - val_accuracy: 0.7185\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.2439 - accuracy: 0.9159 - val_loss: 0.9237 - val_accuracy: 0.7625\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.2266 - accuracy: 0.9198 - val_loss: 1.2062 - val_accuracy: 0.7243\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 23s 237ms/step - loss: 0.2180 - accuracy: 0.9250 - val_loss: 1.0894 - val_accuracy: 0.7155\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.2077 - accuracy: 0.9293 - val_loss: 1.1676 - val_accuracy: 0.7155\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.1947 - accuracy: 0.9381 - val_loss: 1.0968 - val_accuracy: 0.7361\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.2084 - accuracy: 0.9293 - val_loss: 1.0220 - val_accuracy: 0.7449\n",
      "12/12 [==============================] - 2s 147ms/step - loss: 1.2081 - accuracy: 0.7419\n",
      "Test accuracy: 0.7419354915618896\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "96392cf5-745a-4197-897f-10cc04ee1f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "48/48 [==============================] - 23s 446ms/step - loss: 2.0053 - accuracy: 0.3149 - val_loss: 1.8200 - val_accuracy: 0.3284\n",
      "Epoch 2/30\n",
      "48/48 [==============================] - 20s 426ms/step - loss: 1.6120 - accuracy: 0.4553 - val_loss: 1.5307 - val_accuracy: 0.4868\n",
      "Epoch 3/30\n",
      "48/48 [==============================] - 22s 450ms/step - loss: 1.3391 - accuracy: 0.5482 - val_loss: 1.2615 - val_accuracy: 0.5836\n",
      "Epoch 4/30\n",
      "48/48 [==============================] - 21s 441ms/step - loss: 1.1368 - accuracy: 0.6115 - val_loss: 1.2776 - val_accuracy: 0.5953\n",
      "Epoch 5/30\n",
      "48/48 [==============================] - 25s 531ms/step - loss: 1.0028 - accuracy: 0.6626 - val_loss: 1.1581 - val_accuracy: 0.6217\n",
      "Epoch 6/30\n",
      "48/48 [==============================] - 24s 509ms/step - loss: 0.9046 - accuracy: 0.6978 - val_loss: 1.0501 - val_accuracy: 0.6422\n",
      "Epoch 7/30\n",
      "48/48 [==============================] - 23s 486ms/step - loss: 0.8426 - accuracy: 0.7122 - val_loss: 1.0257 - val_accuracy: 0.6364\n",
      "Epoch 8/30\n",
      "48/48 [==============================] - 21s 427ms/step - loss: 0.7785 - accuracy: 0.7321 - val_loss: 0.9750 - val_accuracy: 0.6745\n",
      "Epoch 9/30\n",
      "48/48 [==============================] - 21s 446ms/step - loss: 0.6986 - accuracy: 0.7565 - val_loss: 0.8869 - val_accuracy: 0.6686\n",
      "Epoch 10/30\n",
      "48/48 [==============================] - 22s 459ms/step - loss: 0.6833 - accuracy: 0.7588 - val_loss: 0.8105 - val_accuracy: 0.7038\n",
      "Epoch 11/30\n",
      "48/48 [==============================] - 23s 474ms/step - loss: 0.6419 - accuracy: 0.7699 - val_loss: 0.8208 - val_accuracy: 0.7302\n",
      "Epoch 12/30\n",
      "48/48 [==============================] - 21s 430ms/step - loss: 0.6151 - accuracy: 0.7881 - val_loss: 0.8881 - val_accuracy: 0.7155\n",
      "Epoch 13/30\n",
      "48/48 [==============================] - 20s 421ms/step - loss: 0.5740 - accuracy: 0.8022 - val_loss: 0.8617 - val_accuracy: 0.7155\n",
      "Epoch 14/30\n",
      "48/48 [==============================] - 21s 436ms/step - loss: 0.5197 - accuracy: 0.8198 - val_loss: 1.0078 - val_accuracy: 0.7126\n",
      "Epoch 15/30\n",
      "48/48 [==============================] - 20s 422ms/step - loss: 0.4657 - accuracy: 0.8403 - val_loss: 1.0117 - val_accuracy: 0.6950\n",
      "Epoch 16/30\n",
      "48/48 [==============================] - 21s 447ms/step - loss: 0.4577 - accuracy: 0.8325 - val_loss: 1.0042 - val_accuracy: 0.7155\n",
      "Epoch 17/30\n",
      "48/48 [==============================] - 21s 439ms/step - loss: 0.4558 - accuracy: 0.8328 - val_loss: 0.8409 - val_accuracy: 0.7302\n",
      "Epoch 18/30\n",
      "48/48 [==============================] - 21s 445ms/step - loss: 0.4136 - accuracy: 0.8615 - val_loss: 0.7992 - val_accuracy: 0.7566\n",
      "Epoch 19/30\n",
      "48/48 [==============================] - 21s 443ms/step - loss: 0.3779 - accuracy: 0.8719 - val_loss: 0.8399 - val_accuracy: 0.7185\n",
      "Epoch 20/30\n",
      "48/48 [==============================] - 21s 430ms/step - loss: 0.3330 - accuracy: 0.8774 - val_loss: 0.9591 - val_accuracy: 0.7566\n",
      "Epoch 21/30\n",
      "48/48 [==============================] - 21s 443ms/step - loss: 0.3507 - accuracy: 0.8817 - val_loss: 1.0470 - val_accuracy: 0.6921\n",
      "Epoch 22/30\n",
      "48/48 [==============================] - 23s 472ms/step - loss: 0.2956 - accuracy: 0.8980 - val_loss: 0.9754 - val_accuracy: 0.7654\n",
      "Epoch 23/30\n",
      "48/48 [==============================] - 21s 436ms/step - loss: 0.2939 - accuracy: 0.8973 - val_loss: 0.9573 - val_accuracy: 0.7273\n",
      "Epoch 24/30\n",
      "48/48 [==============================] - 21s 443ms/step - loss: 0.2913 - accuracy: 0.8983 - val_loss: 1.0512 - val_accuracy: 0.7185\n",
      "Epoch 25/30\n",
      "48/48 [==============================] - 21s 431ms/step - loss: 0.2515 - accuracy: 0.9172 - val_loss: 1.1031 - val_accuracy: 0.7214\n",
      "Epoch 26/30\n",
      "48/48 [==============================] - 20s 425ms/step - loss: 0.2479 - accuracy: 0.9091 - val_loss: 1.1284 - val_accuracy: 0.7449\n",
      "Epoch 27/30\n",
      "48/48 [==============================] - 22s 461ms/step - loss: 0.2355 - accuracy: 0.9169 - val_loss: 0.9606 - val_accuracy: 0.7566\n",
      "Epoch 28/30\n",
      "48/48 [==============================] - 21s 446ms/step - loss: 0.2119 - accuracy: 0.9289 - val_loss: 1.1906 - val_accuracy: 0.7331\n",
      "Epoch 29/30\n",
      "48/48 [==============================] - 22s 454ms/step - loss: 0.1986 - accuracy: 0.9384 - val_loss: 1.2014 - val_accuracy: 0.7390\n",
      "Epoch 30/30\n",
      "48/48 [==============================] - 21s 427ms/step - loss: 0.1791 - accuracy: 0.9413 - val_loss: 1.1344 - val_accuracy: 0.7449\n",
      "6/6 [==============================] - 2s 313ms/step - loss: 1.2279 - accuracy: 0.7204\n",
      "Test accuracy: 0.7204301357269287\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=64, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=64, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=64, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa54a684-8a3c-4fa0-8533-5abdf426b3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 2.1013 - accuracy: 0.2898 - val_loss: 2.1119 - val_accuracy: 0.3079\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 1.8373 - accuracy: 0.3892 - val_loss: 1.7051 - val_accuracy: 0.4340\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 24s 245ms/step - loss: 1.5899 - accuracy: 0.4697 - val_loss: 1.6073 - val_accuracy: 0.4457\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 23s 243ms/step - loss: 1.4223 - accuracy: 0.5166 - val_loss: 1.4074 - val_accuracy: 0.5396\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 22s 231ms/step - loss: 1.2855 - accuracy: 0.5707 - val_loss: 1.3128 - val_accuracy: 0.5718\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 1.1661 - accuracy: 0.5997 - val_loss: 1.1660 - val_accuracy: 0.6129\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 1.0976 - accuracy: 0.6196 - val_loss: 1.1732 - val_accuracy: 0.6070\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 24s 249ms/step - loss: 1.0278 - accuracy: 0.6506 - val_loss: 1.1662 - val_accuracy: 0.6012\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 24s 251ms/step - loss: 0.9416 - accuracy: 0.6757 - val_loss: 1.1518 - val_accuracy: 0.6070\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 24s 246ms/step - loss: 0.8995 - accuracy: 0.6959 - val_loss: 0.9616 - val_accuracy: 0.6569\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.7960 - accuracy: 0.7239 - val_loss: 1.0080 - val_accuracy: 0.6305\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.7788 - accuracy: 0.7233 - val_loss: 0.9149 - val_accuracy: 0.7009\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.7423 - accuracy: 0.7422 - val_loss: 0.8602 - val_accuracy: 0.6833\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 23s 240ms/step - loss: 0.6995 - accuracy: 0.7621 - val_loss: 0.9674 - val_accuracy: 0.6716\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.6831 - accuracy: 0.7581 - val_loss: 0.8707 - val_accuracy: 0.7097\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.6331 - accuracy: 0.7754 - val_loss: 0.9499 - val_accuracy: 0.7038\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.6087 - accuracy: 0.7829 - val_loss: 0.9589 - val_accuracy: 0.7155\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.5734 - accuracy: 0.7986 - val_loss: 0.9389 - val_accuracy: 0.6862\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.5426 - accuracy: 0.8116 - val_loss: 0.9473 - val_accuracy: 0.7302\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 19s 195ms/step - loss: 0.5281 - accuracy: 0.8158 - val_loss: 0.9023 - val_accuracy: 0.7185\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.5126 - accuracy: 0.8214 - val_loss: 0.8218 - val_accuracy: 0.7683\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.4637 - accuracy: 0.8419 - val_loss: 0.9892 - val_accuracy: 0.7185\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.4427 - accuracy: 0.8445 - val_loss: 0.9285 - val_accuracy: 0.7331\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.4391 - accuracy: 0.8494 - val_loss: 0.8827 - val_accuracy: 0.7361\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 23s 242ms/step - loss: 0.4136 - accuracy: 0.8510 - val_loss: 1.0064 - val_accuracy: 0.7126\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 22s 231ms/step - loss: 0.3877 - accuracy: 0.8592 - val_loss: 0.8997 - val_accuracy: 0.7625\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.3921 - accuracy: 0.8631 - val_loss: 0.9907 - val_accuracy: 0.7155\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.3553 - accuracy: 0.8739 - val_loss: 1.0548 - val_accuracy: 0.6950\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 21s 216ms/step - loss: 0.3275 - accuracy: 0.8895 - val_loss: 0.9665 - val_accuracy: 0.7361\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.3301 - accuracy: 0.8807 - val_loss: 0.9595 - val_accuracy: 0.7537\n",
      "12/12 [==============================] - 2s 129ms/step - loss: 1.2384 - accuracy: 0.6909\n",
      "Test accuracy: 0.6908602118492126\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9eb83cb4-9369-46d0-b7b3-dbbfdfb95d13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 22s 217ms/step - loss: 1.9738 - accuracy: 0.3312 - val_loss: 1.6847 - val_accuracy: 0.4311\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 1.5299 - accuracy: 0.4827 - val_loss: 1.4036 - val_accuracy: 0.4985\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 23s 235ms/step - loss: 1.2459 - accuracy: 0.5808 - val_loss: 1.1977 - val_accuracy: 0.5894\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 1.0572 - accuracy: 0.6460 - val_loss: 1.0918 - val_accuracy: 0.6598\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.9465 - accuracy: 0.6832 - val_loss: 1.0301 - val_accuracy: 0.6452\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.8247 - accuracy: 0.7138 - val_loss: 0.9338 - val_accuracy: 0.6862\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.7836 - accuracy: 0.7285 - val_loss: 0.9471 - val_accuracy: 0.6628\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.7079 - accuracy: 0.7588 - val_loss: 0.8017 - val_accuracy: 0.7302\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.6534 - accuracy: 0.7647 - val_loss: 1.0835 - val_accuracy: 0.6862\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.5734 - accuracy: 0.8015 - val_loss: 0.9681 - val_accuracy: 0.6716\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.5611 - accuracy: 0.8051 - val_loss: 0.8655 - val_accuracy: 0.7361\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.5131 - accuracy: 0.8292 - val_loss: 0.9651 - val_accuracy: 0.7449\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.4628 - accuracy: 0.8370 - val_loss: 0.9007 - val_accuracy: 0.7302\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.4282 - accuracy: 0.8546 - val_loss: 0.8574 - val_accuracy: 0.7419\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.4064 - accuracy: 0.8550 - val_loss: 1.3052 - val_accuracy: 0.6833\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.4045 - accuracy: 0.8579 - val_loss: 0.8727 - val_accuracy: 0.7214\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.3562 - accuracy: 0.8814 - val_loss: 0.9961 - val_accuracy: 0.7038\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.3560 - accuracy: 0.8817 - val_loss: 1.0397 - val_accuracy: 0.7155\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.3217 - accuracy: 0.8928 - val_loss: 0.9361 - val_accuracy: 0.7185\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.3037 - accuracy: 0.9029 - val_loss: 1.2596 - val_accuracy: 0.6921\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.2840 - accuracy: 0.9087 - val_loss: 1.2182 - val_accuracy: 0.7097\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.2395 - accuracy: 0.9175 - val_loss: 1.2452 - val_accuracy: 0.7126\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.2500 - accuracy: 0.9172 - val_loss: 1.0764 - val_accuracy: 0.7390\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.2342 - accuracy: 0.9211 - val_loss: 1.3037 - val_accuracy: 0.7243\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 22s 231ms/step - loss: 0.2150 - accuracy: 0.9329 - val_loss: 1.1949 - val_accuracy: 0.7595\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.2093 - accuracy: 0.9293 - val_loss: 1.3216 - val_accuracy: 0.7331\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.2073 - accuracy: 0.9299 - val_loss: 1.3797 - val_accuracy: 0.6979\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.1811 - accuracy: 0.9387 - val_loss: 1.3387 - val_accuracy: 0.7243\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.1993 - accuracy: 0.9332 - val_loss: 1.3169 - val_accuracy: 0.7331\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.1790 - accuracy: 0.9449 - val_loss: 1.3850 - val_accuracy: 0.7361\n",
      "12/12 [==============================] - 2s 143ms/step - loss: 1.7630 - accuracy: 0.7151\n",
      "Test accuracy: 0.7150537371635437\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models, optimizers\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "rmsprop = optimizers.RMSprop(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4f6f4617-38d1-4fdb-8ee1-d05f2da29cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/50\n",
      "96/96 [==============================] - 25s 254ms/step - loss: 1.9901 - accuracy: 0.3145 - val_loss: 1.7471 - val_accuracy: 0.4047\n",
      "Epoch 2/50\n",
      "96/96 [==============================] - 24s 251ms/step - loss: 1.5320 - accuracy: 0.4912 - val_loss: 1.3789 - val_accuracy: 0.5425\n",
      "Epoch 3/50\n",
      "96/96 [==============================] - 23s 235ms/step - loss: 1.2401 - accuracy: 0.5903 - val_loss: 1.4789 - val_accuracy: 0.5073\n",
      "Epoch 4/50\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 1.0457 - accuracy: 0.6463 - val_loss: 1.1305 - val_accuracy: 0.6070\n",
      "Epoch 5/50\n",
      "96/96 [==============================] - 22s 232ms/step - loss: 0.9305 - accuracy: 0.6708 - val_loss: 1.1267 - val_accuracy: 0.6070\n",
      "Epoch 6/50\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.8518 - accuracy: 0.7151 - val_loss: 1.0387 - val_accuracy: 0.6569\n",
      "Epoch 7/50\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.7848 - accuracy: 0.7366 - val_loss: 0.9593 - val_accuracy: 0.6628\n",
      "Epoch 8/50\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.7155 - accuracy: 0.7536 - val_loss: 0.8731 - val_accuracy: 0.7067\n",
      "Epoch 9/50\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.6403 - accuracy: 0.7800 - val_loss: 0.9872 - val_accuracy: 0.7009\n",
      "Epoch 10/50\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.6048 - accuracy: 0.7875 - val_loss: 0.8582 - val_accuracy: 0.7097\n",
      "Epoch 11/50\n",
      "96/96 [==============================] - 23s 236ms/step - loss: 0.5534 - accuracy: 0.8093 - val_loss: 0.7996 - val_accuracy: 0.7273\n",
      "Epoch 12/50\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.5222 - accuracy: 0.8230 - val_loss: 0.9052 - val_accuracy: 0.7185\n",
      "Epoch 13/50\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.4629 - accuracy: 0.8282 - val_loss: 1.0230 - val_accuracy: 0.6657\n",
      "Epoch 14/50\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.4729 - accuracy: 0.8318 - val_loss: 0.8429 - val_accuracy: 0.7331\n",
      "Epoch 15/50\n",
      "96/96 [==============================] - 20s 204ms/step - loss: 0.4185 - accuracy: 0.8543 - val_loss: 0.8641 - val_accuracy: 0.7419\n",
      "Epoch 16/50\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.3917 - accuracy: 0.8657 - val_loss: 0.8657 - val_accuracy: 0.7214\n",
      "Epoch 17/50\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.3667 - accuracy: 0.8664 - val_loss: 0.8076 - val_accuracy: 0.7390\n",
      "Epoch 18/50\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.3526 - accuracy: 0.8827 - val_loss: 0.9926 - val_accuracy: 0.7097\n",
      "Epoch 19/50\n",
      "96/96 [==============================] - 21s 216ms/step - loss: 0.3290 - accuracy: 0.8840 - val_loss: 0.9315 - val_accuracy: 0.7390\n",
      "Epoch 20/50\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.3092 - accuracy: 0.8944 - val_loss: 0.9136 - val_accuracy: 0.7566\n",
      "Epoch 21/50\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.2827 - accuracy: 0.9048 - val_loss: 1.0773 - val_accuracy: 0.7302\n",
      "Epoch 22/50\n",
      "96/96 [==============================] - 21s 216ms/step - loss: 0.2754 - accuracy: 0.9055 - val_loss: 1.2211 - val_accuracy: 0.7038\n",
      "Epoch 23/50\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.2423 - accuracy: 0.9153 - val_loss: 1.0428 - val_accuracy: 0.7243\n",
      "Epoch 24/50\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.2303 - accuracy: 0.9175 - val_loss: 1.1170 - val_accuracy: 0.7273\n",
      "Epoch 25/50\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.2598 - accuracy: 0.9143 - val_loss: 1.0668 - val_accuracy: 0.7185\n",
      "Epoch 26/50\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.2128 - accuracy: 0.9299 - val_loss: 1.1557 - val_accuracy: 0.7361\n",
      "Epoch 27/50\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.2175 - accuracy: 0.9208 - val_loss: 1.1565 - val_accuracy: 0.7273\n",
      "Epoch 28/50\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.2082 - accuracy: 0.9299 - val_loss: 0.9847 - val_accuracy: 0.7361\n",
      "Epoch 29/50\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.1991 - accuracy: 0.9260 - val_loss: 1.1710 - val_accuracy: 0.7419\n",
      "Epoch 30/50\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.1626 - accuracy: 0.9390 - val_loss: 1.1731 - val_accuracy: 0.7419\n",
      "Epoch 31/50\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.1610 - accuracy: 0.9469 - val_loss: 1.1885 - val_accuracy: 0.7595\n",
      "Epoch 32/50\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.1687 - accuracy: 0.9355 - val_loss: 1.3046 - val_accuracy: 0.7185\n",
      "Epoch 33/50\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.1658 - accuracy: 0.9449 - val_loss: 1.2564 - val_accuracy: 0.7243\n",
      "Epoch 34/50\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.1544 - accuracy: 0.9521 - val_loss: 1.4952 - val_accuracy: 0.7214\n",
      "Epoch 35/50\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.1425 - accuracy: 0.9537 - val_loss: 1.2639 - val_accuracy: 0.7243\n",
      "Epoch 36/50\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.1430 - accuracy: 0.9511 - val_loss: 1.2412 - val_accuracy: 0.7390\n",
      "Epoch 37/50\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.1547 - accuracy: 0.9459 - val_loss: 1.3012 - val_accuracy: 0.7419\n",
      "Epoch 38/50\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.1301 - accuracy: 0.9580 - val_loss: 1.4048 - val_accuracy: 0.7302\n",
      "Epoch 39/50\n",
      "96/96 [==============================] - 19s 195ms/step - loss: 0.1265 - accuracy: 0.9589 - val_loss: 1.4184 - val_accuracy: 0.7478\n",
      "Epoch 40/50\n",
      "96/96 [==============================] - 23s 239ms/step - loss: 0.1018 - accuracy: 0.9651 - val_loss: 1.5715 - val_accuracy: 0.7273\n",
      "Epoch 41/50\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.1373 - accuracy: 0.9570 - val_loss: 1.3963 - val_accuracy: 0.7243\n",
      "Epoch 42/50\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.1581 - accuracy: 0.9449 - val_loss: 1.5759 - val_accuracy: 0.6950\n",
      "Epoch 43/50\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.1411 - accuracy: 0.9537 - val_loss: 1.2992 - val_accuracy: 0.7273\n",
      "Epoch 44/50\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.0975 - accuracy: 0.9674 - val_loss: 1.4690 - val_accuracy: 0.7126\n",
      "Epoch 45/50\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.1031 - accuracy: 0.9638 - val_loss: 1.3836 - val_accuracy: 0.7243\n",
      "Epoch 46/50\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.1227 - accuracy: 0.9599 - val_loss: 1.4493 - val_accuracy: 0.7273\n",
      "Epoch 47/50\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.0817 - accuracy: 0.9710 - val_loss: 1.5817 - val_accuracy: 0.7390\n",
      "Epoch 48/50\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.1180 - accuracy: 0.9625 - val_loss: 1.4934 - val_accuracy: 0.7243\n",
      "Epoch 49/50\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.1040 - accuracy: 0.9710 - val_loss: 1.3868 - val_accuracy: 0.6950\n",
      "Epoch 50/50\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.0879 - accuracy: 0.9687 - val_loss: 1.4973 - val_accuracy: 0.7097\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 1.5194 - accuracy: 0.7366\n",
      "Test accuracy: 0.7365591526031494\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=50, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e3e08b96-9399-449b-9f8a-40421ae61c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/100\n",
      "96/96 [==============================] - 22s 223ms/step - loss: 2.0310 - accuracy: 0.3103 - val_loss: 1.8339 - val_accuracy: 0.3695\n",
      "Epoch 2/100\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 1.5412 - accuracy: 0.4785 - val_loss: 1.4627 - val_accuracy: 0.4897\n",
      "Epoch 3/100\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 1.2582 - accuracy: 0.5740 - val_loss: 1.2208 - val_accuracy: 0.5748\n",
      "Epoch 4/100\n",
      "96/96 [==============================] - 20s 212ms/step - loss: 1.0613 - accuracy: 0.6372 - val_loss: 1.1344 - val_accuracy: 0.6100\n",
      "Epoch 5/100\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 1.0105 - accuracy: 0.6669 - val_loss: 1.0259 - val_accuracy: 0.6481\n",
      "Epoch 6/100\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.9008 - accuracy: 0.6887 - val_loss: 0.9692 - val_accuracy: 0.6510\n",
      "Epoch 7/100\n",
      "96/96 [==============================] - 22s 232ms/step - loss: 0.7871 - accuracy: 0.7282 - val_loss: 1.0305 - val_accuracy: 0.6481\n",
      "Epoch 8/100\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.7484 - accuracy: 0.7428 - val_loss: 0.9296 - val_accuracy: 0.6804\n",
      "Epoch 9/100\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.6962 - accuracy: 0.7523 - val_loss: 0.8562 - val_accuracy: 0.6891\n",
      "Epoch 10/100\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 0.6501 - accuracy: 0.7839 - val_loss: 0.8735 - val_accuracy: 0.7273\n",
      "Epoch 11/100\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.6052 - accuracy: 0.7937 - val_loss: 0.9701 - val_accuracy: 0.6891\n",
      "Epoch 12/100\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.5466 - accuracy: 0.8025 - val_loss: 0.7800 - val_accuracy: 0.7361\n",
      "Epoch 13/100\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.5096 - accuracy: 0.8230 - val_loss: 0.8515 - val_accuracy: 0.7243\n",
      "Epoch 14/100\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.4748 - accuracy: 0.8380 - val_loss: 0.8886 - val_accuracy: 0.7155\n",
      "Epoch 15/100\n",
      "96/96 [==============================] - 22s 234ms/step - loss: 0.4642 - accuracy: 0.8364 - val_loss: 0.8966 - val_accuracy: 0.7126\n",
      "Epoch 16/100\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.4260 - accuracy: 0.8514 - val_loss: 0.8974 - val_accuracy: 0.7390\n",
      "Epoch 17/100\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.3747 - accuracy: 0.8742 - val_loss: 0.9860 - val_accuracy: 0.7214\n",
      "Epoch 18/100\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 0.3658 - accuracy: 0.8748 - val_loss: 1.0200 - val_accuracy: 0.7654\n",
      "Epoch 19/100\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 0.3268 - accuracy: 0.8820 - val_loss: 1.1717 - val_accuracy: 0.6950\n",
      "Epoch 20/100\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.3103 - accuracy: 0.8866 - val_loss: 0.9370 - val_accuracy: 0.7185\n",
      "Epoch 21/100\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.3044 - accuracy: 0.8872 - val_loss: 0.9090 - val_accuracy: 0.7449\n",
      "Epoch 22/100\n",
      "96/96 [==============================] - 21s 213ms/step - loss: 0.2828 - accuracy: 0.9032 - val_loss: 1.0023 - val_accuracy: 0.7361\n",
      "Epoch 23/100\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 0.2787 - accuracy: 0.9104 - val_loss: 1.0218 - val_accuracy: 0.7449\n",
      "Epoch 24/100\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.2875 - accuracy: 0.8967 - val_loss: 1.0596 - val_accuracy: 0.7155\n",
      "Epoch 25/100\n",
      "96/96 [==============================] - 22s 225ms/step - loss: 0.2382 - accuracy: 0.9214 - val_loss: 1.1694 - val_accuracy: 0.7331\n",
      "Epoch 26/100\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.2234 - accuracy: 0.9234 - val_loss: 1.0532 - val_accuracy: 0.7625\n",
      "Epoch 27/100\n",
      "96/96 [==============================] - 23s 240ms/step - loss: 0.2192 - accuracy: 0.9224 - val_loss: 1.2802 - val_accuracy: 0.7243\n",
      "Epoch 28/100\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.2340 - accuracy: 0.9201 - val_loss: 1.0952 - val_accuracy: 0.7185\n",
      "Epoch 29/100\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.1834 - accuracy: 0.9358 - val_loss: 1.1591 - val_accuracy: 0.7478\n",
      "Epoch 30/100\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.1725 - accuracy: 0.9413 - val_loss: 1.1522 - val_accuracy: 0.7419\n",
      "Epoch 31/100\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.1640 - accuracy: 0.9400 - val_loss: 1.3709 - val_accuracy: 0.7097\n",
      "Epoch 32/100\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.1801 - accuracy: 0.9364 - val_loss: 1.3751 - val_accuracy: 0.7449\n",
      "Epoch 33/100\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.1783 - accuracy: 0.9420 - val_loss: 1.2963 - val_accuracy: 0.7126\n",
      "Epoch 34/100\n",
      "96/96 [==============================] - 20s 213ms/step - loss: 0.1546 - accuracy: 0.9433 - val_loss: 1.4156 - val_accuracy: 0.7214\n",
      "Epoch 35/100\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.1578 - accuracy: 0.9465 - val_loss: 1.3374 - val_accuracy: 0.7155\n",
      "Epoch 36/100\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.1485 - accuracy: 0.9492 - val_loss: 1.8086 - val_accuracy: 0.7067\n",
      "Epoch 37/100\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.1600 - accuracy: 0.9469 - val_loss: 1.2185 - val_accuracy: 0.7273\n",
      "Epoch 38/100\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.1406 - accuracy: 0.9521 - val_loss: 1.3643 - val_accuracy: 0.7449\n",
      "Epoch 39/100\n",
      "96/96 [==============================] - 22s 232ms/step - loss: 0.1148 - accuracy: 0.9622 - val_loss: 1.4703 - val_accuracy: 0.7126\n",
      "Epoch 40/100\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 0.1229 - accuracy: 0.9576 - val_loss: 1.6806 - val_accuracy: 0.7185\n",
      "Epoch 41/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.1475 - accuracy: 0.9531 - val_loss: 1.5137 - val_accuracy: 0.7097\n",
      "Epoch 42/100\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.1467 - accuracy: 0.9475 - val_loss: 1.5000 - val_accuracy: 0.7185\n",
      "Epoch 43/100\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.1053 - accuracy: 0.9648 - val_loss: 1.5392 - val_accuracy: 0.7390\n",
      "Epoch 44/100\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.1097 - accuracy: 0.9628 - val_loss: 1.3954 - val_accuracy: 0.7449\n",
      "Epoch 45/100\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.0958 - accuracy: 0.9668 - val_loss: 1.3728 - val_accuracy: 0.7478\n",
      "Epoch 46/100\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.1116 - accuracy: 0.9635 - val_loss: 1.4600 - val_accuracy: 0.7449\n",
      "Epoch 47/100\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.1214 - accuracy: 0.9632 - val_loss: 1.5013 - val_accuracy: 0.7390\n",
      "Epoch 48/100\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.0787 - accuracy: 0.9749 - val_loss: 1.6126 - val_accuracy: 0.7214\n",
      "Epoch 49/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.0941 - accuracy: 0.9690 - val_loss: 1.7322 - val_accuracy: 0.7243\n",
      "Epoch 50/100\n",
      "96/96 [==============================] - 22s 232ms/step - loss: 0.1002 - accuracy: 0.9687 - val_loss: 1.5215 - val_accuracy: 0.7126\n",
      "Epoch 51/100\n",
      "96/96 [==============================] - 20s 204ms/step - loss: 0.0946 - accuracy: 0.9687 - val_loss: 1.4926 - val_accuracy: 0.7155\n",
      "Epoch 52/100\n",
      "96/96 [==============================] - 21s 213ms/step - loss: 0.0842 - accuracy: 0.9710 - val_loss: 1.7417 - val_accuracy: 0.6862\n",
      "Epoch 53/100\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.0884 - accuracy: 0.9710 - val_loss: 1.5622 - val_accuracy: 0.7185\n",
      "Epoch 54/100\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.1089 - accuracy: 0.9664 - val_loss: 1.4716 - val_accuracy: 0.7361\n",
      "Epoch 55/100\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.0897 - accuracy: 0.9703 - val_loss: 1.4216 - val_accuracy: 0.7243\n",
      "Epoch 56/100\n",
      "96/96 [==============================] - 23s 241ms/step - loss: 0.0874 - accuracy: 0.9687 - val_loss: 1.6221 - val_accuracy: 0.7390\n",
      "Epoch 57/100\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.1029 - accuracy: 0.9664 - val_loss: 1.6159 - val_accuracy: 0.7243\n",
      "Epoch 58/100\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.0786 - accuracy: 0.9726 - val_loss: 1.6094 - val_accuracy: 0.7478\n",
      "Epoch 59/100\n",
      "96/96 [==============================] - 21s 213ms/step - loss: 0.0997 - accuracy: 0.9684 - val_loss: 1.6170 - val_accuracy: 0.7243\n",
      "Epoch 60/100\n",
      "96/96 [==============================] - 21s 224ms/step - loss: 0.0867 - accuracy: 0.9733 - val_loss: 1.6310 - val_accuracy: 0.7361\n",
      "Epoch 61/100\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.1084 - accuracy: 0.9651 - val_loss: 1.6769 - val_accuracy: 0.7009\n",
      "Epoch 62/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.1291 - accuracy: 0.9593 - val_loss: 1.4183 - val_accuracy: 0.7038\n",
      "Epoch 63/100\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.0845 - accuracy: 0.9743 - val_loss: 1.5657 - val_accuracy: 0.7273\n",
      "Epoch 64/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.0768 - accuracy: 0.9765 - val_loss: 1.7093 - val_accuracy: 0.7361\n",
      "Epoch 65/100\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.0543 - accuracy: 0.9837 - val_loss: 1.6349 - val_accuracy: 0.7067\n",
      "Epoch 66/100\n",
      "96/96 [==============================] - 21s 213ms/step - loss: 0.0597 - accuracy: 0.9778 - val_loss: 1.9310 - val_accuracy: 0.7273\n",
      "Epoch 67/100\n",
      "96/96 [==============================] - 22s 230ms/step - loss: 0.0752 - accuracy: 0.9772 - val_loss: 1.5765 - val_accuracy: 0.7361\n",
      "Epoch 68/100\n",
      "96/96 [==============================] - 23s 235ms/step - loss: 0.0634 - accuracy: 0.9824 - val_loss: 1.6535 - val_accuracy: 0.7331\n",
      "Epoch 69/100\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.0518 - accuracy: 0.9850 - val_loss: 1.7565 - val_accuracy: 0.7185\n",
      "Epoch 70/100\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.0639 - accuracy: 0.9801 - val_loss: 1.7651 - val_accuracy: 0.7302\n",
      "Epoch 71/100\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.0635 - accuracy: 0.9788 - val_loss: 1.7051 - val_accuracy: 0.7302\n",
      "Epoch 72/100\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.0571 - accuracy: 0.9844 - val_loss: 1.8101 - val_accuracy: 0.7507\n",
      "Epoch 73/100\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.0649 - accuracy: 0.9804 - val_loss: 1.9347 - val_accuracy: 0.7067\n",
      "Epoch 74/100\n",
      "96/96 [==============================] - 23s 236ms/step - loss: 0.0827 - accuracy: 0.9778 - val_loss: 1.7158 - val_accuracy: 0.7449\n",
      "Epoch 75/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.0577 - accuracy: 0.9808 - val_loss: 1.8162 - val_accuracy: 0.6979\n",
      "Epoch 76/100\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.0626 - accuracy: 0.9798 - val_loss: 1.9510 - val_accuracy: 0.7214\n",
      "Epoch 77/100\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.0959 - accuracy: 0.9661 - val_loss: 1.9620 - val_accuracy: 0.7155\n",
      "Epoch 78/100\n",
      "96/96 [==============================] - 22s 231ms/step - loss: 0.0848 - accuracy: 0.9736 - val_loss: 1.8604 - val_accuracy: 0.7126\n",
      "Epoch 79/100\n",
      "96/96 [==============================] - 20s 212ms/step - loss: 0.0755 - accuracy: 0.9765 - val_loss: 1.8184 - val_accuracy: 0.7273\n",
      "Epoch 80/100\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.0601 - accuracy: 0.9801 - val_loss: 2.0346 - val_accuracy: 0.7097\n",
      "Epoch 81/100\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.0565 - accuracy: 0.9811 - val_loss: 1.9218 - val_accuracy: 0.7185\n",
      "Epoch 82/100\n",
      "96/96 [==============================] - 22s 229ms/step - loss: 0.0470 - accuracy: 0.9866 - val_loss: 2.1669 - val_accuracy: 0.6921\n",
      "Epoch 83/100\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.0649 - accuracy: 0.9814 - val_loss: 2.2366 - val_accuracy: 0.7067\n",
      "Epoch 84/100\n",
      "96/96 [==============================] - 23s 237ms/step - loss: 0.0780 - accuracy: 0.9778 - val_loss: 1.9853 - val_accuracy: 0.7067\n",
      "Epoch 85/100\n",
      "96/96 [==============================] - 22s 230ms/step - loss: 0.0418 - accuracy: 0.9860 - val_loss: 2.2869 - val_accuracy: 0.6979\n",
      "Epoch 86/100\n",
      "96/96 [==============================] - 22s 230ms/step - loss: 0.0526 - accuracy: 0.9811 - val_loss: 2.1381 - val_accuracy: 0.7214\n",
      "Epoch 87/100\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 0.0820 - accuracy: 0.9736 - val_loss: 1.8326 - val_accuracy: 0.6979\n",
      "Epoch 88/100\n",
      "96/96 [==============================] - 19s 200ms/step - loss: 0.0508 - accuracy: 0.9817 - val_loss: 1.9300 - val_accuracy: 0.7126\n",
      "Epoch 89/100\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.0559 - accuracy: 0.9831 - val_loss: 1.8980 - val_accuracy: 0.7419\n",
      "Epoch 90/100\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 0.0505 - accuracy: 0.9811 - val_loss: 2.0244 - val_accuracy: 0.7331\n",
      "Epoch 91/100\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.0519 - accuracy: 0.9850 - val_loss: 2.2002 - val_accuracy: 0.7155\n",
      "Epoch 92/100\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.0626 - accuracy: 0.9821 - val_loss: 1.9948 - val_accuracy: 0.7185\n",
      "Epoch 93/100\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.0871 - accuracy: 0.9749 - val_loss: 2.0196 - val_accuracy: 0.7273\n",
      "Epoch 94/100\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.0545 - accuracy: 0.9827 - val_loss: 2.0628 - val_accuracy: 0.7155\n",
      "Epoch 95/100\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.0699 - accuracy: 0.9811 - val_loss: 1.9534 - val_accuracy: 0.7273\n",
      "Epoch 96/100\n",
      "96/96 [==============================] - 22s 227ms/step - loss: 0.0640 - accuracy: 0.9791 - val_loss: 2.2227 - val_accuracy: 0.7273\n",
      "Epoch 97/100\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 0.0663 - accuracy: 0.9788 - val_loss: 1.9549 - val_accuracy: 0.7243\n",
      "Epoch 98/100\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.0454 - accuracy: 0.9870 - val_loss: 2.2222 - val_accuracy: 0.6979\n",
      "Epoch 99/100\n",
      "96/96 [==============================] - 20s 204ms/step - loss: 0.0503 - accuracy: 0.9831 - val_loss: 2.1649 - val_accuracy: 0.7185\n",
      "Epoch 100/100\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.0622 - accuracy: 0.9778 - val_loss: 1.8491 - val_accuracy: 0.7243\n",
      "12/12 [==============================] - 1s 119ms/step - loss: 2.0217 - accuracy: 0.7258\n",
      "Test accuracy: 0.725806474685669\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=100, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cdef7cfc-254b-4f29-a464-60ac6f6fef19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 23s 228ms/step - loss: 2.1347 - accuracy: 0.2572 - val_loss: 2.1249 - val_accuracy: 0.2375\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 20s 211ms/step - loss: 1.9097 - accuracy: 0.3582 - val_loss: 1.7246 - val_accuracy: 0.4252\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 22s 224ms/step - loss: 1.5443 - accuracy: 0.4791 - val_loss: 1.4968 - val_accuracy: 0.4780\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 1.3583 - accuracy: 0.5300 - val_loss: 1.2822 - val_accuracy: 0.5865\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 1.2163 - accuracy: 0.5874 - val_loss: 1.2091 - val_accuracy: 0.5836\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 1.1328 - accuracy: 0.6226 - val_loss: 1.0765 - val_accuracy: 0.6334\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 1.0304 - accuracy: 0.6535 - val_loss: 1.0780 - val_accuracy: 0.6129\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 19s 194ms/step - loss: 0.9922 - accuracy: 0.6607 - val_loss: 1.0404 - val_accuracy: 0.6452\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 20s 206ms/step - loss: 0.9316 - accuracy: 0.6848 - val_loss: 0.9672 - val_accuracy: 0.6716\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 20s 204ms/step - loss: 0.8820 - accuracy: 0.7040 - val_loss: 0.9887 - val_accuracy: 0.6628\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 0.8537 - accuracy: 0.7066 - val_loss: 0.9981 - val_accuracy: 0.6598\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 19s 194ms/step - loss: 0.8223 - accuracy: 0.7177 - val_loss: 0.9404 - val_accuracy: 0.6686\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.8014 - accuracy: 0.7269 - val_loss: 0.9169 - val_accuracy: 0.6891\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.7503 - accuracy: 0.7396 - val_loss: 0.9615 - val_accuracy: 0.6862\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 21s 217ms/step - loss: 0.7399 - accuracy: 0.7464 - val_loss: 0.8727 - val_accuracy: 0.7038\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.7025 - accuracy: 0.7581 - val_loss: 0.9240 - val_accuracy: 0.6921\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 19s 195ms/step - loss: 0.6831 - accuracy: 0.7634 - val_loss: 0.8429 - val_accuracy: 0.7097\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 19s 194ms/step - loss: 0.6726 - accuracy: 0.7702 - val_loss: 0.9594 - val_accuracy: 0.6804\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.6508 - accuracy: 0.7702 - val_loss: 0.9085 - val_accuracy: 0.7009\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.6067 - accuracy: 0.7855 - val_loss: 0.8659 - val_accuracy: 0.7067\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 23s 235ms/step - loss: 0.6123 - accuracy: 0.7953 - val_loss: 0.8540 - val_accuracy: 0.7302\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 20s 212ms/step - loss: 0.5859 - accuracy: 0.7953 - val_loss: 0.8098 - val_accuracy: 0.7273\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.5447 - accuracy: 0.8123 - val_loss: 0.9082 - val_accuracy: 0.7214\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.5414 - accuracy: 0.8136 - val_loss: 0.7984 - val_accuracy: 0.7273\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 0.4942 - accuracy: 0.8331 - val_loss: 0.8458 - val_accuracy: 0.7155\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.4992 - accuracy: 0.8334 - val_loss: 0.8258 - val_accuracy: 0.7419\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.4793 - accuracy: 0.8387 - val_loss: 0.8603 - val_accuracy: 0.7243\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 21s 215ms/step - loss: 0.4713 - accuracy: 0.8400 - val_loss: 0.8778 - val_accuracy: 0.7067\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.4413 - accuracy: 0.8497 - val_loss: 0.8012 - val_accuracy: 0.7331\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 19s 193ms/step - loss: 0.4308 - accuracy: 0.8546 - val_loss: 0.8726 - val_accuracy: 0.7067\n",
      "12/12 [==============================] - 2s 139ms/step - loss: 0.8585 - accuracy: 0.7204\n",
      "Test accuracy: 0.7204301357269287\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='softmax'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "25b8e00c-5fda-4f87-b244-c32b3601bbe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 22s 217ms/step - loss: 2.1459 - accuracy: 0.2565 - val_loss: 2.1668 - val_accuracy: 0.2375\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 2.0998 - accuracy: 0.2722 - val_loss: 1.9938 - val_accuracy: 0.3138\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 23s 238ms/step - loss: 1.7729 - accuracy: 0.4006 - val_loss: 1.6783 - val_accuracy: 0.4370\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 1.5385 - accuracy: 0.4769 - val_loss: 1.5220 - val_accuracy: 0.5044\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 1.3920 - accuracy: 0.5244 - val_loss: 1.4883 - val_accuracy: 0.5044\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 1.2831 - accuracy: 0.5619 - val_loss: 1.2911 - val_accuracy: 0.5630\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 1.1606 - accuracy: 0.6017 - val_loss: 1.1859 - val_accuracy: 0.6012\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 19s 200ms/step - loss: 1.0656 - accuracy: 0.6242 - val_loss: 1.1085 - val_accuracy: 0.6012\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 1.0118 - accuracy: 0.6529 - val_loss: 1.1377 - val_accuracy: 0.6188\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.9590 - accuracy: 0.6633 - val_loss: 1.0382 - val_accuracy: 0.6188\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.8787 - accuracy: 0.6877 - val_loss: 0.9854 - val_accuracy: 0.6393\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 19s 195ms/step - loss: 0.8558 - accuracy: 0.6995 - val_loss: 1.0103 - val_accuracy: 0.6276\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 19s 197ms/step - loss: 0.8116 - accuracy: 0.7168 - val_loss: 1.0029 - val_accuracy: 0.6569\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 19s 199ms/step - loss: 0.7676 - accuracy: 0.7327 - val_loss: 0.9603 - val_accuracy: 0.6540\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 20s 204ms/step - loss: 0.7432 - accuracy: 0.7438 - val_loss: 0.9225 - val_accuracy: 0.6628\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.6907 - accuracy: 0.7630 - val_loss: 0.9572 - val_accuracy: 0.6598\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.6946 - accuracy: 0.7627 - val_loss: 1.0197 - val_accuracy: 0.6686\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 19s 193ms/step - loss: 0.6554 - accuracy: 0.7712 - val_loss: 0.9395 - val_accuracy: 0.6628\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.6034 - accuracy: 0.7940 - val_loss: 0.9699 - val_accuracy: 0.6598\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.5608 - accuracy: 0.7940 - val_loss: 0.9523 - val_accuracy: 0.6833\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 22s 233ms/step - loss: 0.5355 - accuracy: 0.8096 - val_loss: 0.9363 - val_accuracy: 0.6979\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.5448 - accuracy: 0.8106 - val_loss: 0.9884 - val_accuracy: 0.6804\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 19s 196ms/step - loss: 0.4883 - accuracy: 0.8302 - val_loss: 1.0041 - val_accuracy: 0.6862\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 0.4840 - accuracy: 0.8292 - val_loss: 1.1035 - val_accuracy: 0.6716\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 19s 200ms/step - loss: 0.4453 - accuracy: 0.8396 - val_loss: 1.1253 - val_accuracy: 0.6628\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 21s 214ms/step - loss: 0.4339 - accuracy: 0.8494 - val_loss: 1.0947 - val_accuracy: 0.6862\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 21s 223ms/step - loss: 0.4153 - accuracy: 0.8576 - val_loss: 1.0681 - val_accuracy: 0.6891\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 0.3920 - accuracy: 0.8608 - val_loss: 0.9756 - val_accuracy: 0.7155\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.3824 - accuracy: 0.8664 - val_loss: 1.0550 - val_accuracy: 0.6891\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 19s 198ms/step - loss: 0.3665 - accuracy: 0.8713 - val_loss: 1.1192 - val_accuracy: 0.6950\n",
      "12/12 [==============================] - 2s 123ms/step - loss: 1.2057 - accuracy: 0.6532\n",
      "Test accuracy: 0.6532257795333862\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='softmax'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b07664ed-43ba-41e6-b941-20c2680e833f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 10 classes.\n",
      "Found 341 images belonging to 10 classes.\n",
      "Found 372 images belonging to 10 classes.\n",
      "Epoch 1/30\n",
      "96/96 [==============================] - 22s 212ms/step - loss: 2.0212 - accuracy: 0.3093 - val_loss: 1.8620 - val_accuracy: 0.3636\n",
      "Epoch 2/30\n",
      "96/96 [==============================] - 21s 222ms/step - loss: 1.7131 - accuracy: 0.4237 - val_loss: 1.6825 - val_accuracy: 0.4164\n",
      "Epoch 3/30\n",
      "96/96 [==============================] - 19s 202ms/step - loss: 1.4785 - accuracy: 0.4919 - val_loss: 1.4705 - val_accuracy: 0.4897\n",
      "Epoch 4/30\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 1.3061 - accuracy: 0.5508 - val_loss: 1.3490 - val_accuracy: 0.5425\n",
      "Epoch 5/30\n",
      "96/96 [==============================] - 20s 207ms/step - loss: 1.1952 - accuracy: 0.5883 - val_loss: 1.2538 - val_accuracy: 0.5748\n",
      "Epoch 6/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 1.0987 - accuracy: 0.6274 - val_loss: 1.1989 - val_accuracy: 0.5924\n",
      "Epoch 7/30\n",
      "96/96 [==============================] - 20s 212ms/step - loss: 1.0388 - accuracy: 0.6395 - val_loss: 1.1402 - val_accuracy: 0.6129\n",
      "Epoch 8/30\n",
      "96/96 [==============================] - 24s 247ms/step - loss: 1.0069 - accuracy: 0.6542 - val_loss: 1.1400 - val_accuracy: 0.5953\n",
      "Epoch 9/30\n",
      "96/96 [==============================] - 26s 272ms/step - loss: 0.9519 - accuracy: 0.6685 - val_loss: 1.1296 - val_accuracy: 0.5836\n",
      "Epoch 10/30\n",
      "96/96 [==============================] - 23s 238ms/step - loss: 0.9158 - accuracy: 0.6822 - val_loss: 1.1566 - val_accuracy: 0.5924\n",
      "Epoch 11/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.8853 - accuracy: 0.6975 - val_loss: 1.0991 - val_accuracy: 0.6305\n",
      "Epoch 12/30\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 0.8523 - accuracy: 0.7145 - val_loss: 1.0907 - val_accuracy: 0.6129\n",
      "Epoch 13/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.7892 - accuracy: 0.7311 - val_loss: 1.0499 - val_accuracy: 0.6452\n",
      "Epoch 14/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.7964 - accuracy: 0.7256 - val_loss: 1.0374 - val_accuracy: 0.6774\n",
      "Epoch 15/30\n",
      "96/96 [==============================] - 20s 208ms/step - loss: 0.7552 - accuracy: 0.7370 - val_loss: 1.0467 - val_accuracy: 0.6393\n",
      "Epoch 16/30\n",
      "96/96 [==============================] - 22s 228ms/step - loss: 0.7202 - accuracy: 0.7549 - val_loss: 1.0095 - val_accuracy: 0.6510\n",
      "Epoch 17/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.6912 - accuracy: 0.7683 - val_loss: 1.0623 - val_accuracy: 0.6569\n",
      "Epoch 18/30\n",
      "96/96 [==============================] - 20s 203ms/step - loss: 0.6693 - accuracy: 0.7689 - val_loss: 1.1157 - val_accuracy: 0.6217\n",
      "Epoch 19/30\n",
      "96/96 [==============================] - 21s 216ms/step - loss: 0.6497 - accuracy: 0.7728 - val_loss: 1.0521 - val_accuracy: 0.6510\n",
      "Epoch 20/30\n",
      "96/96 [==============================] - 21s 219ms/step - loss: 0.6179 - accuracy: 0.7875 - val_loss: 1.0427 - val_accuracy: 0.6540\n",
      "Epoch 21/30\n",
      "96/96 [==============================] - 22s 230ms/step - loss: 0.5954 - accuracy: 0.7907 - val_loss: 1.1453 - val_accuracy: 0.6129\n",
      "Epoch 22/30\n",
      "96/96 [==============================] - 20s 209ms/step - loss: 0.5634 - accuracy: 0.7999 - val_loss: 1.0530 - val_accuracy: 0.6158\n",
      "Epoch 23/30\n",
      "96/96 [==============================] - 20s 205ms/step - loss: 0.5501 - accuracy: 0.8158 - val_loss: 1.1220 - val_accuracy: 0.6246\n",
      "Epoch 24/30\n",
      "96/96 [==============================] - 19s 201ms/step - loss: 0.5188 - accuracy: 0.8269 - val_loss: 1.1096 - val_accuracy: 0.6217\n",
      "Epoch 25/30\n",
      "96/96 [==============================] - 22s 226ms/step - loss: 0.5195 - accuracy: 0.8204 - val_loss: 1.0731 - val_accuracy: 0.6452\n",
      "Epoch 26/30\n",
      "96/96 [==============================] - 24s 246ms/step - loss: 0.4903 - accuracy: 0.8331 - val_loss: 1.0548 - val_accuracy: 0.6774\n",
      "Epoch 27/30\n",
      "96/96 [==============================] - 21s 220ms/step - loss: 0.4686 - accuracy: 0.8370 - val_loss: 1.1413 - val_accuracy: 0.6393\n",
      "Epoch 28/30\n",
      "96/96 [==============================] - 21s 221ms/step - loss: 0.4644 - accuracy: 0.8360 - val_loss: 1.0928 - val_accuracy: 0.6305\n",
      "Epoch 29/30\n",
      "96/96 [==============================] - 21s 218ms/step - loss: 0.4276 - accuracy: 0.8595 - val_loss: 1.0400 - val_accuracy: 0.6774\n",
      "Epoch 30/30\n",
      "96/96 [==============================] - 20s 210ms/step - loss: 0.4039 - accuracy: 0.8608 - val_loss: 1.2202 - val_accuracy: 0.6540\n",
      "12/12 [==============================] - 2s 151ms/step - loss: 1.4715 - accuracy: 0.5860\n",
      "Test accuracy: 0.5860214829444885\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from PIL import Image\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "train_data = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
    "val_data = ImageDataGenerator(rescale=1./255)\n",
    "test_data = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory('data/clothing-dataset-small-master/train/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "val_generator = val_datagen.flow_from_directory('data/clothing-dataset-small-master/validation', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "test_generator = test_datagen.flow_from_directory('data/clothing-dataset-small-master/test/', target_size=(64, 64), batch_size=32, class_mode='categorical')\n",
    "\n",
    "# Define the CNN model\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, kernel_size=(7, 7), activation='relu', input_shape=(64, 64, 3)),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(64, kernel_size=(7, 7), activation='relu'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Conv2D(128, kernel_size=(3, 3), activation='softmax'),\n",
    "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_generator, epochs=30, validation_data=val_generator)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_acc = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
